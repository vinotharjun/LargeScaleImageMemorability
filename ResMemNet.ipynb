{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResMemNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNZL0Shx4978wjeFhoyxuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinotharjun/LargeScaleImageMemorability/blob/master/ResMemNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1zIO4D0EE9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from skimage import io,transform\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms, utils,models\n",
        "import copy\n",
        "import math\n",
        "from skimage import io, transform\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCkduQUREr_4",
        "colab_type": "code",
        "outputId": "8cdc6237-75b6-4376-fde4-9e630b79cb18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twH0DeNQVx4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuvYrf_sEu5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i973_G4cEcA2",
        "colab_type": "code",
        "outputId": "406702a4-5e79-4ba1-ddf1-afd1074af535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://memorability.csail.mit.edu/lamem.tar.gz\n",
        "!tar -xf /content/lamem.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-31 06:47:56--  http://memorability.csail.mit.edu/lamem.tar.gz\n",
            "Resolving memorability.csail.mit.edu (memorability.csail.mit.edu)... 128.30.195.49\n",
            "Connecting to memorability.csail.mit.edu (memorability.csail.mit.edu)|128.30.195.49|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2708368436 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘lamem.tar.gz’\n",
            "\n",
            "lamem.tar.gz        100%[===================>]   2.52G  41.3MB/s    in 63s     \n",
            "\n",
            "2020-03-31 06:49:00 (40.7 MB/s) - ‘lamem.tar.gz’ saved [2708368436/2708368436]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzpUAtwE8HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = pd.read_csv(\"/content/drive/My Drive/image memorability/dataset/train_dataset.csv\")\n",
        "dataset_validation = pd.read_csv(\"/content/drive/My Drive/image memorability/dataset/val.csv\")\n",
        "dataset_train = dataset_train[:1000]\n",
        "dataset_validation = dataset_validation[:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv1cRgN9vSaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhnQkk_vbP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb7GK82bFJz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AsetheticsDataset(Dataset):\n",
        "      '''asethitics dataset'''\n",
        "      def __init__(self,dataframe,root_dir,transform=None):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                csv_file (string): Path to the csv file with annotations.\n",
        "                root_dir (string): Directory with all the images.\n",
        "                transform (callable, optional): Optional transform to be applied\n",
        "                    on a sample.\n",
        "        \"\"\"\n",
        "        self.data = dataframe\n",
        "        #     self.data.rename(columns=columns,inplace=True)\n",
        "    #     self.data.drop(self.data.columns[[1,2,3,4,5,6,8,9]] , axis=1,inplace=True)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "      def __len__(self):\n",
        "        return len(self.data)\n",
        "  \n",
        "      def __getitem__(self,idx):\n",
        "   \n",
        "        if torch.is_tensor(idx):\n",
        "          idx = idx.tolist()\n",
        "\n",
        "        image_name =  os.path.join(self.root_dir,self.data.iloc[idx,0])\n",
        "        image = io.imread(image_name)\n",
        "        mem_val = self.data.iloc[idx,1]\n",
        "#     return_sample={}\n",
        "        return_sample = {\n",
        "              'image':image,\n",
        "              'memorability_score':mem_val \n",
        "        }\n",
        "        if self.transform:\n",
        "            return_sample = self.transform(return_sample)\n",
        "    \n",
        "     \n",
        "        return return_sample\n",
        "\n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \n",
        "        image,mem_val = sample['image'], sample[\"memorability_score\"]\n",
        "        \n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        # if isinstance(self.output_size, int):\n",
        "        #     if h > w:\n",
        "        #         new_h, new_w = self.output_size * h / w, self.output_size\n",
        "        #     else:\n",
        "        #         new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        # else:\n",
        "        #     new_h, new_w = self.output_size\n",
        "\n",
        "        # new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (self.output_size,self.output_size,3))\n",
        "        return {'image': img, 'memorability_score': mem_val}\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \n",
        "        image, mem_val = sample['image'], sample['memorability_score']\n",
        "#         print(type(torch.from_numpy(image)))\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "#         print(image.shape)\n",
        "      \n",
        "        image = image.transpose((2, 0, 1))\n",
        "        \n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'memorability_score': mem_val}\n",
        "class Normalize(object):\n",
        "    def __init__(self,mean,std):\n",
        "        self.mean=mean\n",
        "        self.std=std\n",
        "    def __call__(self,sample):\n",
        "        image, mem_val = sample[\"image\"], sample[\"memorability_score\"]\n",
        "        normalized=  (image -self.mean) / self.std\n",
        "        return {\n",
        "            \"image\":normalized,\n",
        "            \"memorability_score\" : mem_val\n",
        "       }\n",
        "\n",
        "transformed_dataset_train = AsetheticsDataset(dataset_train,root_dir=\"/content/lamem/images\",\n",
        "                                        transform=transforms.Compose([Rescale(28),ToTensor(),Normalize(0.5,0.5)\n",
        "                                                          ]))\n",
        "\n",
        "transformed_dataset_val= AsetheticsDataset(dataset_validation,root_dir=\"/content/lamem/images\",\n",
        "                                        transform=transforms.Compose([Rescale(28),ToTensor(),Normalize(0.5,0.5)\n",
        "                                                          ]))\n",
        "\n",
        "train_dataloader=DataLoader(transformed_dataset_train,batch_size=32,shuffle=True)\n",
        "validation_dataloader=DataLoader(transformed_dataset_val,batch_size=32,shuffle=True)\n",
        "dataloaders={\n",
        "    \"train\":train_dataloader,\n",
        "    \"val\":validation_dataloader\n",
        "}\n",
        "dataset_sizes ={\n",
        "    \"train\":len(dataset_train),\n",
        "    \"val\":len(dataset_validation)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTxMkclfCm_2",
        "colab_type": "code",
        "outputId": "805ba97f-65bc-4eb4-b3e4-b87d90835f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(transformed_dataset_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uH9i0MQFcYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionLayer(nn.Module):\n",
        "    '''\n",
        "    Args:\n",
        "    in_channels = the number of channels of input channel (type int)\n",
        "    output_channels = the number of output feature maps (type int)\n",
        "    kernel_size = the size of the kernel (type tuple)\n",
        "    strides = the length of the strides (type int)\n",
        "    padding = takes two 1) valid, 2) same or some other integer value [default:\"valid\"]\n",
        "    \n",
        "    '''\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=(3,3),strides=1,padding=\"valid\"):\n",
        "        super(ConvolutionLayer,self).__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels= out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding     = padding\n",
        "        self.strides     = strides\n",
        "        #layer definition\n",
        "        self.convolution_layer = self.conv_layer(in_channels,out_channels,kernel_size,strides,padding)\n",
        "        self.batch_normalize = nn.BatchNorm2d(out_channels)\n",
        "        self.relu       = nn.ReLU()\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.convolution_layer(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.batch_normalize(x)\n",
        "        return x\n",
        "    \n",
        "    def conv_layer(self,in_channels,out_channels,kernel_size,strides,padding):\n",
        "        if padding == \"valid\":\n",
        "            padding =0\n",
        "        elif padding == \"same\":\n",
        "            strides = 1\n",
        "            padding = math.floor(int((kernel_size[0]-1)/2))  \n",
        "        return nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDTzDJgfFdHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LstmCell(nn.Module):\n",
        "    '''\n",
        "    Args:\n",
        "    input_dims : the input dimension (that takes single integer) (type int)\n",
        "    hidden dims : the dimension of hidden and cell state (type int)\n",
        "    attach_fc  : to ensure whether fully connected layer should be connected to last rnn cell [default:False]\n",
        "    '''\n",
        "    def __init__(self,input_dims,hidden_dims,attach_fc = False):\n",
        "        super(LstmCell,self).__init__()\n",
        "        \n",
        "        self.input_dims = input_dims\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.attach_fc = attach_fc\n",
        "        \n",
        "        self.lstm_cell = nn.LSTMCell(input_size = self.input_dims,hidden_size = self.hidden_dims)\n",
        "        if self.attach_fc == True:\n",
        "            self.fc = nn.Linear(self.hidden_dims,1)\n",
        "        \n",
        "    def forward(self,x,hidden_state,cell_state):\n",
        "        hidden_output,cell_output = self.lstm_cell(x,(hidden_state,cell_state))\n",
        "        if self.attach_fc ==True:\n",
        "            output = self.fc(hidden_output)\n",
        "            return output\n",
        "        else:\n",
        "            return hidden_output,cell_output\n",
        "        \n",
        "#     def init_hidden(self, batch_size):\n",
        "#         hidden = torch.tensor(next(self.parameters()).data.new(batch_size, self.hidden_dims), requires_grad=False)\n",
        "#         cell = torch.tensor(next(self.parameters()).data.new(batch_size, self.hidden_dims), requires_grad=False)\n",
        "#         return hidden.zero_(), cell.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxCBtt4DFgbm",
        "colab_type": "code",
        "outputId": "a716ba08-9c15-447a-a499-70f72223c12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class VRNet(nn.Module):\n",
        "    def __init__(self,in_channels,cnn_layer,lstm_cell):\n",
        "        super(VRNet,self).__init__()\n",
        "        self.hidden_dims =128\n",
        "        self.input_dims =128\n",
        "        self.stage_lstm =lstm_cell(self.input_dims,self.hidden_dims)\n",
        "        #stage1\n",
        "        self.stage1_cnn = cnn_layer(in_channels=3,out_channels=32,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage1_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage1 inter\n",
        "        self.stage1_inter_cnn3x3 = cnn_layer(in_channels=32,out_channels=64,kernel_size=(3,3),padding=\"valid\",strides=1)\n",
        "        self.stage1_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"valid\",strides=1)\n",
        "        self.stage1_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "       \n",
        "\n",
        "        #stage2\n",
        "        self.stage2_cnn = cnn_layer(in_channels=32,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage2_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage2 inter\n",
        "        self.stage2_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage2_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage2_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage3\n",
        "        self.stage3_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage3_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage3 inter\n",
        "        self.stage3_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage3_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage3_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage4\n",
        "        self.stage4_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage4_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage4 inter\n",
        "        self.stage4_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage4_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage4_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage5\n",
        "        self.stage5_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage5_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage5 inter\n",
        "        self.stage5_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage5_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage5_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage6\n",
        "        self.stage6_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage6_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage6 inter\n",
        "        self.stage6_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage6_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage6_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage7\n",
        "        self.stage7_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage7_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage6 inter\n",
        "        self.stage7_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage7_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage7_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        #stage8\n",
        "        self.stage8_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        # self.stage8_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage6 inter\n",
        "        self.stage8_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage8_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage8_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "        #stage9\n",
        "        self.stage9_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage9_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage6 inter\n",
        "        self.stage9_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage9_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage9_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "        #stage7\n",
        "        self.stage10_cnn = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage10_pool = nn.AvgPool2d(kernel_size=(3,3),stride=2)\n",
        "        #stage6 inter\n",
        "        self.stage10_inter_cnn3x3 = cnn_layer(in_channels=64,out_channels=64,kernel_size=(3,3),padding=\"same\",strides=1)\n",
        "        self.stage10_inter_cnn1x1 = cnn_layer(in_channels=64,out_channels=128,kernel_size=(1,1),padding=\"same\",strides=1)\n",
        "        self.stage10_interpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "\n",
        "         #linear\n",
        "        self.fc = nn.Linear(self.hidden_dims,1)\n",
        "        \n",
        "        \n",
        "    def forward(self,x,hidden_state,cell_state):\n",
        "        #stage1\n",
        "        x = self.stage1_cnn(x)\n",
        "        x = self.stage1_pool(x)\n",
        "        \n",
        "        #stage1 inter\n",
        "        x1 = self.stage1_inter_cnn3x3(x)\n",
        "        x1 = self.stage1_inter_cnn1x1(x1)\n",
        "        x1  = self.stage1_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "        \n",
        "        #stage2\n",
        "        x = self.stage2_cnn(x)\n",
        "        # x = self.stage2_pool(x)\n",
        "      \n",
        "        #stage2 inter\n",
        "        x1 = self.stage2_inter_cnn3x3(x)\n",
        "        x1 = self.stage2_inter_cnn1x1(x1)\n",
        "        x1 = self.stage2_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "        #stage3\n",
        "        x = self.stage3_cnn(x)\n",
        "        # x = self.stage3_pool(x)\n",
        "        \n",
        "\n",
        "        #stage4 inter\n",
        "        x1 = self.stage3_inter_cnn3x3(x)\n",
        "        x1 = self.stage3_inter_cnn1x1(x1)\n",
        "        x1 = self.stage3_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "     \n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "        #stage4\n",
        "        x = self.stage4_cnn(x)\n",
        "        # x = self.stage4_pool(x)\n",
        "\n",
        "        #stage4 inter\n",
        "        x1 = self.stage4_inter_cnn3x3(x)\n",
        "        x1 = self.stage4_inter_cnn1x1(x1)\n",
        "        x1 = self.stage4_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "         #stage5\n",
        "        x = self.stage5_cnn(x)\n",
        "        # x = self.stage5_pool(x)\n",
        "\n",
        "        #stage5 inter\n",
        "        x1 = self.stage5_inter_cnn3x3(x)\n",
        "        x1 = self.stage5_inter_cnn1x1(x1)\n",
        "        x1 = self.stage5_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "         #stage6\n",
        "        x = self.stage6_cnn(x)\n",
        "        # x = self.stage6_pool(x)\n",
        "\n",
        "        #stage6 inter\n",
        "        x1 = self.stage6_inter_cnn3x3(x)\n",
        "        x1 = self.stage6_inter_cnn1x1(x1)\n",
        "        x1 = self.stage6_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "          #stage7\n",
        "        x = self.stage7_cnn(x)\n",
        "        # x = self.stage7_pool(x)\n",
        "\n",
        "        #stage7 inter\n",
        "        x1 = self.stage7_inter_cnn3x3(x)\n",
        "        x1 = self.stage7_inter_cnn1x1(x1)\n",
        "        x1 = self.stage7_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "          #stage8\n",
        "        x = self.stage8_cnn(x)\n",
        "        # x = self.stage8_pool(x)\n",
        "\n",
        "        #stage8 inter\n",
        "        x1 = self.stage8_inter_cnn3x3(x)\n",
        "        x1 = self.stage8_inter_cnn1x1(x1)\n",
        "        x1 = self.stage8_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "          #stage9\n",
        "        x = self.stage9_cnn(x)\n",
        "        x = self.stage9_pool(x)\n",
        "\n",
        "        #stage7 inter\n",
        "        x1 = self.stage9_inter_cnn3x3(x)\n",
        "        x1 = self.stage9_inter_cnn1x1(x1)\n",
        "        x1 = self.stage9_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "          #stage10\n",
        "        x = self.stage10_cnn(x)\n",
        "        x = self.stage10_pool(x)\n",
        "\n",
        "        #stage7 inter\n",
        "        x1 = self.stage10_inter_cnn3x3(x)\n",
        "        x1 = self.stage10_inter_cnn1x1(x1)\n",
        "        x1 = self.stage10_interpool(x1)\n",
        "        x1 = x1.squeeze()\n",
        "\n",
        "        hidden_state,cell_state = self.stage_lstm(x1,hidden_state,cell_state)\n",
        "        del x1\n",
        "\n",
        "\n",
        "        return self.fc(hidden_state)\n",
        "        \n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.tensor(next(self.parameters()).data.new(batch_size, self.hidden_dims), requires_grad=False)\n",
        "        cell = torch.tensor(next(self.parameters()).data.new(batch_size, self.hidden_dims), requires_grad=False)\n",
        "        return hidden.zero_(), cell.zero_()\n",
        "\n",
        "model = VRNet(3,ConvolutionLayer,LstmCell).to(device).double()\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "    if isinstance(m,nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "        \n",
        "model.apply(weights_init)\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRuGoIMb2tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t = torch.rand([32,3,224,224]).to(device).double()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJLisgi1b9kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hidden_state ,cell_state = model.init_hidden(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw-IEm87cADb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# with torch.no_grad():\n",
        "#   x =model(t,hidden_state,cell_state)\n",
        "#   print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHssD-0FVaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def getMSE(d1, d2):\n",
        "        mse = 0.0\n",
        "        for a,b in zip(d1, d2):\n",
        "            mse += (a-b)**2\n",
        "        return mse / len(d1)\n",
        "\n",
        "def getRankCorrelation(predicted, gt=None):\n",
        "\n",
        "        if gt is None:\n",
        "            return \"needed gt\"\n",
        "        gt = np.array(gt).tolist()\n",
        "        predicted = np.array(predicted).squeeze().tolist()\n",
        "\n",
        "        n = min(len(predicted), len(gt))\n",
        "        if n < 2:\n",
        "            return 0\n",
        "\n",
        "        gt = gt[:n]\n",
        "        predicted = predicted[:n]\n",
        "        mse = getMSE(gt, predicted)\n",
        "\n",
        "        def get_rank(list_a):\n",
        "            rank_list = np.zeros(len(list_a))\n",
        "            idxs = np.array(list_a).argsort()\n",
        "            for rank, i in enumerate(idxs):\n",
        "                rank_list[i] = rank\n",
        "\n",
        "            return rank_list\n",
        "\n",
        "        gt_rank = get_rank(gt)\n",
        "        predicted_rank = get_rank(predicted)\n",
        "        ssd = 0\n",
        "        for i in range(len(predicted_rank)):\n",
        "            ssd += (gt_rank[i] -  predicted_rank[i])**2\n",
        "\n",
        "        rc = 1-(6*ssd/(n*n*n - n))\n",
        "\n",
        "\n",
        "        return rc, mse\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
        "    since = time.time()\n",
        "    running_loss_history = []\n",
        "    val_running_loss_history=[]\n",
        "    orignal_model=None\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    low_loss = np.inf\n",
        "    stop = False\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "       \n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        if stop == True:\n",
        "          break\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            rank_corr=[]\n",
        "\n",
        "            # Iterate over data.\n",
        "            for batched_data in tqdm(dataloaders[phase]):\n",
        "                \n",
        "                inputs=batched_data[\"image\"]\n",
        "                inputs = inputs.to(device)\n",
        "                hidden_state ,cell_state = model.init_hidden(inputs.size(0))\n",
        "                if inputs.size(0) == 16:\n",
        "                   print(hidden_state.shape)\n",
        "                labels=batched_data[\"memorability_score\"]\n",
        "                labels=labels.view(-1,1).double()\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs,hidden_state,cell_state)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == \"val\":\n",
        "                      rc,mse = getRankCorrelation(outputs.squeeze().tolist(),labels.squeeze().tolist())\n",
        "                      rank_corr.append(rc)\n",
        "                    print(\"  batch loss:    \",loss.item())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "               \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            if phase==\"train\":\n",
        "              running_loss_history.append(epoch_loss)\n",
        "            else:\n",
        "              val_running_loss_history.append(epoch_loss)\n",
        "            \n",
        "\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, epoch_loss))\n",
        "            \n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == \"val\":\n",
        "                rho =sum(rank_corr)/len(rank_corr)\n",
        "                print(\"rank correlation,final\",rho)\n",
        "                if rho >=0.67:\n",
        "                  print(\"rank correlation final\",rho)\n",
        "                  stop = True\n",
        "            \n",
        "            if phase == 'val' and epoch_loss < low_loss:\n",
        "                print(\"saving best model......\")\n",
        "                low_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_model_wts,\"/content/drive/My Drive/image memorability/saved models/resnet50_weights_over.pt\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    # load best model weights\n",
        "    original_model =copy.deepcopy(model)\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model,original_model,running_loss_history,val_running_loss_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PSgszQHv8qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f= open(\"/content/drive/My Drive/image memorability/saved models/resnet50_weights_over.pt\",\"w+\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVB1rW8Ob2H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample =iter(train_dataloader).next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZ8yY6W6EjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# h,c=model.init_hidden(8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT9xDvfocNha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample[\"memorability_score\"].view(-1,1).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24VhhwwPBDAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "        # rc, _ = stats.spearmanr(a=predicted, b=gt, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umRbDF0DXLxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B37DPj2_XnEK",
        "colab_type": "code",
        "outputId": "a045d99f-79c7-4028-884c-d74a62036fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "best_model,_,_ ,_= train_model(best_model,criterion,optimizer,exp_lr_scheduler,100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:231: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 1/32 [00:02<01:29,  2.90s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.006938466257007716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  6%|▋         | 2/32 [00:08<01:48,  3.61s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.013267911606045839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 3/32 [00:11<01:41,  3.49s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01158581663754972\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 12%|█▎        | 4/32 [00:15<01:42,  3.67s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01839847821169457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 5/32 [00:19<01:42,  3.80s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.010877960597763789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 6/32 [00:22<01:33,  3.58s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.009538517723665145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 7/32 [00:25<01:27,  3.49s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01116875338793686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 8/32 [00:28<01:20,  3.34s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.018898556518095272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 9/32 [00:32<01:16,  3.31s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.00595325569823942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 31%|███▏      | 10/32 [00:35<01:09,  3.17s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.015677265411407692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 11/32 [00:38<01:09,  3.32s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.012398704527942544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 38%|███▊      | 12/32 [00:41<01:04,  3.21s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.019986967339910563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 41%|████      | 13/32 [00:45<01:02,  3.30s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.017342121519889873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 44%|████▍     | 14/32 [00:48<00:59,  3.28s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.019678890767970324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 47%|████▋     | 15/32 [00:53<01:06,  3.93s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.018723270783526563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 16/32 [01:00<01:14,  4.68s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.0214878982039744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 53%|█████▎    | 17/32 [01:03<01:02,  4.18s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01260626115572195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 56%|█████▋    | 18/32 [01:16<01:38,  7.01s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.011604055576214587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 59%|█████▉    | 19/32 [01:32<02:05,  9.67s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01193473157490318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 62%|██████▎   | 20/32 [01:36<01:34,  7.84s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01813946370435643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 66%|██████▌   | 21/32 [01:42<01:20,  7.35s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.02317679813263295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 69%|██████▉   | 22/32 [01:45<01:00,  6.01s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.011793862055841195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 72%|███████▏  | 23/32 [01:49<00:48,  5.42s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.014104092685826342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 75%|███████▌  | 24/32 [01:59<00:55,  6.92s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.017095069347702167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 78%|███████▊  | 25/32 [02:10<00:56,  8.04s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01671033185248636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 81%|████████▏ | 26/32 [02:13<00:39,  6.50s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01270776132177065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 84%|████████▍ | 27/32 [02:16<00:27,  5.56s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.014390193472367061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 88%|████████▊ | 28/32 [02:20<00:19,  4.88s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01626180071205715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 91%|█████████ | 29/32 [02:23<00:13,  4.39s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.014829790301724353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 94%|█████████▍| 30/32 [02:26<00:08,  4.03s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.014292543745402862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 97%|█████████▋| 31/32 [02:31<00:04,  4.32s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01556304804160457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 32/32 [02:32<00:00,  4.76s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.033509915924109276\n",
            "train Loss: 0.0149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 1/7 [00:03<00:20,  3.35s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.01034178177386198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 29%|██▊       | 2/7 [00:07<00:17,  3.56s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.007604582945936754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 43%|████▎     | 3/7 [00:12<00:16,  4.01s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.009662935197448309\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 57%|█████▋    | 4/7 [00:15<00:11,  3.69s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.008676476085882182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 71%|███████▏  | 5/7 [00:21<00:08,  4.27s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.005366097618310501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 86%|████████▌ | 6/7 [00:26<00:04,  4.62s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.006766579961967743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 7/7 [00:27<00:00,  3.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  batch loss:     0.0027273751101957395\n",
            "val Loss: 0.0079\n",
            "rank correlation,final 0.6803668681548866\n",
            "rank correlation final 0.6803668681548866\n",
            "saving best model......\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "Training complete in 2m 59s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjg2zrmXXyyc",
        "colab_type": "code",
        "outputId": "eef7a2de-d09d-4610-c5bd-d82256028171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(transformed_dataset_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUkJG7gVXblw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}